\chapter{Literature Review}
\label{chapter:litreview}

The expectation that teachers will be able to identify their impact is an idea that can be found in policy language, but it is not well established in educational literature. This literature review sought to answer the research question:

\textbf{What constitutes good quality evidence of impact in recent educational literature?}

\section{Search strategy}

We conducted a search of literature data bases during October 2019: Informit, ERIC/Ed Source and EBSCO. The abstracts were reviewed and any articles that were not peer reviewed, were not published between 2010-2019, were not in English, and abstracts were not relevant to the Australian HALT context were excluded. Duplicates were removed. The remaining 44 article abstracts were reviewed by the research team and 17 articles were summarised. Table \ref{tab:literature_search} outlines the search parameters.


\begin{table}[h!]
\caption{Literature search parameters}
\label{tab:literature_search}
\tablestyle[qutsansbold]
%
\begin{tabular}{c p{6.5cm} *{4}{c}} %p{0.45\textwidth}
\theadstart
    \thead Date &
    \thead Search Terms &
    \multicolumn{2}{c}{\thead Informit \scriptsize{(A+ Education)}} & % \cline{3-4} &
    \multicolumn{2}{c}{\thead EBSCO} & %\\ \cline{5-6} &
\tsubheadstart
    \tsubhead \footnotesize(2019)& \tsubhead & 
    \tsubhead \footnotesize Results & \tsubhead \footnotesize Relevant & 
    \tsubhead \footnotesize Results & \tsubhead \footnotesize Relevant &
\tbody
 %
  15/10 & Quality Evidence AND Teachers & 
  372 &	9 & 158 &	3 \\
 %
   & Teacher Evaluation AND Evidence & 
  208 &	3 & 528 &	4 \\
%  			
    & Quality Indicators AND Teacher Impact & 
   27 &	0 & 4 &	0 \\
 %
    & Teacher Impact AND Teacher Evaluation & 
   453 & 2 & 306 & 7 \\
 %
    & Teacher Impact AND Teacher Evaluation AND Australia & 
   85 &	2 & 49 &	0 \\
%  			
  17/10  & Quality Indicators AND Teachers AND Portfolios & 
   3 &	1 & & \\
 %
    & Quality Portfolios AND teachers & 
   9 &	2 & 32 & \\
 %
    & Assessing Portfolios & 
  9 &	1 & 29 &	2 \\
 %
    & \raggedright Portfolio assessment AND teacher standards & 
  8 &	2 & 10 &	1 \\
%  		
    & Assessing teachers AND Teacher standards & 
  8 &	1 & 25 &	1 \\
 %
    & Teacher portfolios AND Teacher Standards & 
   17 &	3 &	13 & 2 \\
 %
    & Assessor consistency AND Teach* & 
   0 & & 1 & 0 \\
 %
    & Assessor consistency AND portfolios & 
   0 & & 0 & \\
 %
    & Teacher Impact & 
  792 & & 5143 & \\
%  		
    & Teacher impact AND accomplished teacher & 
   1 &	0 &	3 & \\
 %
    & Quality Evidence AND Teacher Portfolio & 
   2 &	2 &	3 &	2 \\
 %
    & Quality Evidence AND Teacher Standards & 
   39 &	3 &	1 &	0 \\
 %
    & Evaluating Quality AND evidence based practice & 
   1 &	0 &	12 & 0 \\
 %
    & Impact evidence AND teacher standards & 
   9 &	1 &	9 &	0 \\
 %
   29/10 & Teacher Impact AND quality & 
   270 & & 571 & \\
 %
    & Teacher Impact AND quality AND Australia & 
   205 & 6 & 48 & 0 \\
 %
    & Teacher Impact AND Evidence & 
    204 &	3 &	372 & \\
 %
    & Teacher Impact AND Evidence AND Australia &
    & & 18 & 0 \\
    \hline 
 \multicolumn{6}{l}{\tsubhead Final number of papers reviewed in detail:	\textbf{17}} &
 \tend
\end{tabular}
\end{table} 

\section{Key insights from the literature}
\medskip
\subsection{Portfolios are a credible way of collecting evidence of impact}
\medskip
    \begin{enumerate}
        \item Portfolio approaches are established as an effective process for teachers to empower teachers to collect evidence \parencite{shepherdReframingPortfolioEvidence2013}
        \item In Australia, preservice teachers are being trained to develop portfolios of evidence, using artefacts, reflections and commentaries to support their claims \parencite{allardAuthenticallyAssessingGraduate2014}.
        \item Portfolios address concerns about standardized test scores being a narrow measure of teacher effectiveness \parencite{shepherdReframingPortfolioEvidence2013}
    \end{enumerate}

\bigskip	
\subsection{Quality evidence has some clear characteristics}
\medskip

    \begin{enumerate}
        \item Quality evidence is credible – which can be judged by the source of evidence, proximity to the event, the structure and relevance of evidence \parencite{shepherdReframingPortfolioEvidence2013}.
        \item If quality evidence supports causal claims, there is an implication that the inquiry focus, assumptions and intended outcomes need to be clear from the start \parencite{shepherdReframingPortfolioEvidence2013}.
        \item Quality evidence will have construct validity – that is it will be closely related to the inquiry topic under study e.g., student self-efficacy \parencite{vanderschaafExploringRoleAssessment2012}.
        \item Teachers need to be taught more about evidence credibility. Teachers are not usually trained in complex quantitative measures of evaluation. They can learn from specific processes in health or law to build an argument chain or design small scale intervention studies that involve measuring from baseline data. Four examples are given in \textcite{shepherdReframingPortfolioEvidence2013}.
        \item Quality itself can reflect different sets of assumptions. \textcite{churchwardPursuitTeacherQuality2019} identify 6 discourses of quality - Readiness, standards, effectiveness, responsibilisation, performativity, professional identity. These discourses all prioritise different types of evidence. 
    \end{enumerate}

\bigskip  
\subsection{Measurement approaches point to common areas of teacher performance}
\medskip

    \begin{enumerate}
        \item In Australia, \textcite{ingvarsonRecognisingAccomplishedTeachers2010} used four outcome measures to do a regression analysis of survey data to follow up on the impact that teacher professional learning had on teachers 3 months after learning. The domains were teacher:
        \begin{enumerate}
            \item knowledge
            \item practice
            \item student learning and
            \item teacher efficacy.
        \end{enumerate}
        \item \textcite{bellQualitiesClassroomObservation2019} did a systematic review and identified criteria most used to focus on teacher practice, where evidence is usually clustered around:
        \begin{enumerate}
            \item Teaching strategies, behaviour management, assessment for learning, student motivation
            \item Cultural views of quality in the context
            \item Subject discipline contexts and knowledge
            \item Student actions, either affective, cognitive or behavioural changes
            item Time scales, and a validity argument.
        \end{enumerate}
        \item Teachers’ core practices are often categorised into  \parencite{vanderschaafEvidenceMeasuringTeachers2019}:
        \begin{enumerate}
            \item pre-lesson activities, i.e. goal setting, developing learning tasks and lessons
            \item lesson activities, i.e. instructing, guiding, and assessing and
            \item post-lesson activities, i.e. reflecting on one’s own teaching.
        \end{enumerate}
        \item Value added models of teacher impact in the USA have been shown to be problematic, with validity challenges such as:
        \begin{enumerate}
            \item challenges related to the validity of instruments as measures of accomplished teaching, especially in diverse classroom contexts, and
            \item challenges related to connecting accomplished teaching to student learning.
        \end{enumerate}
        \item \textcite{wyatt-smithStandardsPracticeStandards2017} identify that statements of professional standards are used across national and international jurisdictions, and have 3 functions in representing impact:
        \begin{enumerate}
            \item They represent the multiplicity of codified components of the task of teaching
            \item They represent the identification of incremental steps in the proficiency of practice of the standards as progression markers, and
            \item the terms used to convey the standards capture degrees of quality.
        \end{enumerate}
    \end{enumerate}

\bigskip   
\subsection{Assessor judgment making process also influences ideas of evidence quality.}
\medskip

\begin{enumerate}
    \item There has been little research into the processes that assessors of portfolios use to make judgments about the quality of evidence.  \textcite{vanderschaafExploringRoleAssessment2012} draw on work by  \textcite{clarkPersonallySeededDiscussionsScaffold2007} for quality of the support of evidence for arguments
    \begin{enumerate}
        \item No support (level 1)
        \item Using explanation as support (level 2)
        \item Using evidence as support (levels 3 and 4)
        \item Coordinating multiple pieces of evidence or multiple connections between ideas in the evidence (level 5)
    \end{enumerate}
    \item \textcite{ingvarsonTeachingStandardsPromotion2019} reported a 4 point scale used in an ACER study of HALT portfolios in Victoria:
    \begin{description}
        \item[4 points --] evidence more than met the certification level
        \item[3 points --] clear evidence of meeting the standard
        \item[2 points --] there was evidence but it was insufficient
        \item[1 points --] there was little or no evidence
    \end{description}
    
    Assessors made overall judgments by looking for links between: 
    \begin{enumerate}
        \item evidence about the students and the selected learning goals
        \item the learning goals and the learning activities, materials and resources
        \item the learning goals and the methods of monitoring and assessing student learning
        \item the teacher's analysis and reflection of their teaching and the evidence of their students' learning
    \end{enumerate}
    
    \item For assessor judgement to be valid and reliable, moderation activities need to occur, with examples of evidence that are annotated used to reach a shared understanding \parencite{wyatt-smithStandardsPracticeStandards2017}. 
    
    Arguments for impact depend on the theoretical concept of learning and the context.

    \item There are at least 127 measurement tools that focus on teacher quality, but there is a lack of evidence about their reliability or theoretical soundness  \parencite{vanderschaafEvidenceMeasuringTeachers2019}. Observation rating tools and systems all understand teaching in unique and specific ways, privileging one set of teaching practices over another set  \parencite{bellQualitiesClassroomObservation2019}. 
    
    \item \textcite{alteratorEncapsulatingTeacherExpertise2018} draw on \posscite{sfardTwoMetaphorsLearning2008} two metaphors: learning as acquisition, which will privilege knowledge as pre-packaged, objective, resolved, and testable and the teacher as expert casting and enacted design. Learning as participation privileges the learner as a dynamic agent, and will focus on evidence showing how the teacher makes multiple attempts, often repeatedly with slight variations as they seek to engage learners.
    
    \item \textcite{ryanSpatialisedMetaphorsPractice2016} emphasise the context, or spaces in which teachers demonstrate their development.
    
\end{enumerate}

These insights from the literature review informed the design of a 3 hour workshop with HALT applicants. 


%\fix{Need to make sure punctuation is consistent through all lists. Commas, full-stops, colons, semicolons, `and' before last item, are inconsistently used.}
